{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "batchDuration=5   # Batch duration in seconds\n",
    "n=3               # Number of columns of a sample (with the intercept)\n",
    "aVariance1=1      # First model - a of the variance of the estimator V_(0) = aI with a > 0\n",
    "aVariance2=1      # Second model - a of the variance of the estimator V_(0) = aI with a > 0\n",
    "mu1=1.0           # First model - forgetting factor\n",
    "mu2=0.9           # Second model - forgetting factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re, ast\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Streaming test\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\") # \"local[2]\" to run locally with 2 cores\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main entry point for Spark Streaming functionality. A StreamingContext represents the\n",
    "# connection to a Spark cluster, and can be used to create DStream various input sources.\n",
    "# A Discretized Stream (DStream), the basic abstraction in Spark Streaming, \n",
    "# is a continuous sequence of RDDs (of the same type) representing a continuous stream of data\n",
    "# DStreams can either be created from live data (such as, data from TCP sockets, Kafka, Flume, etc.) using a \n",
    "# StreamingContext or it can be generated by transforming existing DStreams using operations such as map and window.\n",
    "ssc = StreamingContext(sc, batchDuration) \n",
    "\n",
    "# Sets the context to periodically checkpoint the DStream operations for master fault-tolerance.\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Set each DStreams in this context to remember RDDs it generated in the last given duration. \n",
    "ssc.remember(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {'volatility': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readInput(line):\n",
    "    # Converts the input string into a pair of numbers\n",
    "    print('now')\n",
    "    vec= [float(x) for x in line.split()] \n",
    "\n",
    "    # The pair represents the inputs for two model (forgetting factor 1 and 0.99)\n",
    "    return [('mod1',('mod1',np.array(vec))),('mod2',('mod2',np.array(vec)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## RLS update function\n",
    "    ## new_values = (key, [i, y, proxies])\n",
    "    ## state = what is given by sc.parallelize in next cell on the first call\n",
    "    ##       = what this function returned on the other calls\n",
    "    if (len(new_values)>0 ):\n",
    "        \n",
    "        key=new_values[0][0]\n",
    "        yx=new_values[0][1]\n",
    "        i=yx[0]\n",
    "        y=yx[1]\n",
    "        x=yx[2:]\n",
    "        n=len(x)\n",
    "        \n",
    "        beta=state[1]\n",
    "        beta.shape=(n,1) # Transorms beta in a matrix of n rows and 1 column\n",
    "        V=state[2]\n",
    "        mu=state[3]\n",
    "        sse=state[4]  ## Sum of squared errors\n",
    "        N=state[5]    ## Number of treated samples\n",
    "        x.shape=(1,n)\n",
    "        err=y-x.dot(beta)        \n",
    "        sse=sse+pow(err,2.0)\n",
    "        V=1.0/mu*(V-V.dot(x.T).dot(x).dot(V)/(1.0+float(x.dot(V).dot(x.T)))) # dot = matrix multiplication\n",
    "                                                                             # .T = Transpose\n",
    "        gamma=V.dot(x.T)\n",
    "        beta=beta+gamma*err\n",
    "        proxyPrediction=x.dot(beta)\n",
    "        \n",
    "        errNaiveLast=y-x[0][-1]\n",
    "        errNaiveMean=y-np.mean(x[0][1:])\n",
    "        sseNaiveLast=state[7]\n",
    "        sseNaiveMean=state[8]\n",
    "        sseNaiveLast=sseNaiveLast+pow(errNaiveLast,2.0)\n",
    "        sseNaiveMean=sseNaiveMean+pow(errNaiveMean,2.0)\n",
    "        \n",
    "        MSE_RLS=sse/(N+1.0)\n",
    "        MSE_NaiveLast=sseNaiveLast/(N+1.0)\n",
    "        MSE_NaiveMean=sseNaiveMean/(N+1.0)\n",
    "        \n",
    "        if (key=='mod1'):\n",
    "            return (key,beta,V,mu,MSE_RLS,N+1, proxyPrediction, sseNaiveLast, sseNaiveMean, MSE_RLS/MSE_NaiveLast, MSE_RLS/MSE_NaiveMean, i)  ## update formula mod1\n",
    "        else:\n",
    "            return (key,beta,V,mu,MSE_RLS,N+1, proxyPrediction, sseNaiveLast, sseNaiveMean, MSE_RLS/MSE_NaiveLast, MSE_RLS/MSE_NaiveMean, i)  ## update formula mod2\n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, ast\n",
    "\n",
    "beta1=np.zeros(n)  ## Initial parameter vector\n",
    "beta2=np.zeros(n)\n",
    "V1=np.diag(np.zeros(n)+aVariance1) ## Initial covariance matrix\n",
    "V2=np.diag(np.zeros(n)+aVariance2)\n",
    "\n",
    "# Returns a new DStream by applying a function to each element of DStream.\n",
    "data = kvs.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "\n",
    "# Returns a new DStream by applying a function to all elements of this DStream, and then flattening the results\n",
    "data=data.flatMap(lambda x: [('mod1',('mod1',1.0*np.array(x))),\n",
    "                            ('mod2',('mod2',1.0*np.array(x)))])\n",
    "\n",
    "# The elements of the collection [] are copied to form a distributed dataset that can be operated on in parallel.\n",
    "initialStateRDD = sc.parallelize([(u'mod1', ('mod1',beta1,V1,mu1,0,0,0,0,0,0)),\n",
    "                                  (u'mod2', ('mod2',beta2,V2,mu2,0,0,0,0,0,0))])\n",
    "\n",
    "# Returns a new \"state\" DStream where the state for each key is updated by applying the given function on the previous\n",
    "# state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.\n",
    "data2=data.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "\n",
    "# Displays key, beta, proxyPrediction, MSE_RLS, NMSE_last and NMSE_mean\n",
    "data2.map(lambda x: [\"Samples received: \", x[1][11], \"Key: \", x[1][0], \"Coefficients: \", np.array2string((x[1][1].T)[0]), \"Prediction: \", np.array2string(x[1][6][0]), \"MSE_RLS: \", np.array2string(x[1][4][0]), \"NMSE_last: \", np.array2string(x[1][9][0]), \"NMSE_mean: \", np.array2string(x[1][10][0])]).pprint() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts the reception and the forecasting\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stops the reception and the forecasting\n",
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
